# Gestione della memoria centrale

## Tecniche di base di primo livello

### Indirizzamento in memoria centrale

#### Indirizzi logici e indirizzi fisici
Quando scriviamo un programma e ne generiamo il codice eseguibile, il programma P ha un suo spazio di indirizzamento in cui pone: il codice, i dati globali, lo heap e lo stack.

Quando il programma vuole accedere ad uno specifico indirizzo (eg. 100hex) si dice che questo e' l'**indirizzo logico** della cella di memoria posta in tale locazione: tale indirizzo viene calcolato a partire dall'indirizzo zero di base del programma P (dal primo byte occupato dal programma nel suo spazio di indirizzamento).

Questo pero' non coincide con lo spazio di indirizzamento fisico in cui il programma si trova.

In memoria centrale, dove abbiamo vari programmi caricati, ogni programma ha uno spazio di indirizzamento riservato in modo tale che ognuno di questi possa evolvere nella sua computazione.
In particolare, il programma P, potra' essere caricato a partire dalla posizione 500hex in memoria centrale: questo indirizzo viene anche chiamato **indirizzo di base**.

La cella di memoria con indirizzo logico 100hex, rispetto al programma P, si trovera' in un **indirizzo fisico** in memoria centrale. Per ottenere l'indirizzo fisico di tale cella bisognera' calcolare l'*offset* in memoria centrale a partire dall'indirizzo 0hex.

![indirizzamento-memoria-centrale](md_resources/M1_indirizzamento-memoria-centrale.jpeg)

#### Collegamento degli indirizzi logici agli indirizzi fisici
Durante la fase di scrittura del programma, utilizzeremo delle descrizioni *simboliche* per riferirci a delle celle di memoria, che il *compilatore e il linker tradurranno poi in indirizzi numerici in memoria fisica*.

Le operazioni di **linking** da una descrizione simbolica ad un indirizzo logico nella memoria del programma vengono effettuate dal compilatore e dal linker, definendo cosi' lo spazio di indirizzamento logico per quel programma e i valori logici di ciascuna variabile, procedura, ecc.

L'operazione di *caricamento* effettuera' il **binding** definitivo ponendo il valore fisico associato al valore logico.

Il collegamento degli indirizzi, inteso come associazione di un valore logico ad un indirizzo fisico di una cella di memoria, puo' avvenire in 3 momenti specifici:
* In fase di compilazione
* In fase di caricamento
* In fase di esecuzione


#### Binding statico - Fase di compilazione
In **fase di compilazione** noi forniamo un *indirizzo di base* in memoria fisica che indica l'indirizzo da cui vogliamo far partire lo spazio di allocazione del programma da noi compilato.
I riferimenti simbolici degli indirizzi, all'interno del codice eseguibile, verrano resi indirizzi fisici eseguendo un semplice calcolo:

> Ponendo:
> * IF : Indirizzo fisico di una cella di memoria indirizzata dal programma;
> * IL : Indirizzo logico di una cella di memoria indirizzata dal programma;
> * IB : Indirizzo di base da cui viene caricato lo spazio di indirizzamento del programma;
> * 0hex : Indirizzo di partenza della memoria fisica;

> IF = 0hex + IB + IL

L'indirizzamento viene dunque **caricato staticamente all'interno del codice**, ed ogni volta che il programma verra' caricato, sara' caricato sempre a quell'indirizzo di base.


#### Binding statico - Fase di caricamento
In **fase di caricamento** in quanto abbiamo ancora la possibilita' di spostare il programma *rilocandolo* a partire da un indirizzo di base diverso.

Cambiando indirizzo di base da cui far partire lo spazio di indirizzamento di un programma abbiamo la possibilita' di rilocare in posizioni diverse in memoria centrale il codice e quindi, cambiando anche i riferimenti fisici alle celle indirizzate logicamente.

In questo caso si parla di **caricamento statico con rilocazione del codice durante il caricamento**.

#### Binding statico - Fase di esecuzione
In **fase di esecuzione** abbiamo la possibilita' di caricare il programma in una posizione di memoria a partire dal un indirizzo di base, che viene memorizzato in un dispositivo apposito: la **Memory Management Unit**, in modo tale che ogni volta che viene generato un indirizzo logico, questo viene tradotto aggiungendo l'offset di base e quindi andando a generare l'indirizzo fisico desiderato.

La rilocazione in questo caso e' molto facile, in quanto basta assegnare lo spazio di indirizzamento del programma a partire da un altro indirizzo di base e cambiare il contenuto del registro della Memory Management Unit per farlo puntare al nuovo spazio di indirizzamento.

In modo automatico la Memory Management Unit si occupera' di rigenerare tutti gli indirizzi senza che debba essere modificato nulla all'interno del codice ottenendo cosi' un **caricamento statico con rilocazione del codice in esecuzione**.

#### Binding dinamico
E' possibile effettuare il caricamento del programma in maniera **dinamica** per minimizzare l'utilizzo della memoria centrale, effettivamente caricando solo le porzioni che servono. 

Posso ottenere un **caricamento dinamico** quando vado a caricare una porzione di programma che viene messa in esecuzione. Nel momento in cui la porzione di programma fara' riferimento ad un'altra porzione (non presente in memoria centrale) il supporto runtime del linguaggio di programmazione provvedera' a caricare in memoria centrale la porzione di codice necessaria. 

Questo approccio e' tipico quando si vogliono utilizzare librerie condivise. 
Senza il caricamento dinamico nella memoria centrale saremmo costretti a ripetere porzioni di programma per una corretta esecuzione.

![linking_dinamico](md_resources/M1_linking_dinamico.png)
___

### Partizionamento
#### Tecnica del partizionamento della memoria centrale
La memoria centrale deve essere allocata ai programmi e al sistema operativo per consentire ad essi di depositare codice, dati, heap e stack necessari alla loro evoluzione della computazione.

Lo spazio di indirizzamento del processore (ossia il numero di parole fisiche diverse che il processore puo' indirizzare) e' usualmente molto superiore allo spazio fisico di memoria installata nel sistema di elaborazione.

Ciascun processo puo' quindi utilizzare, dello spazio fisico di memoria, una porzione al piu' grande come lo spazio di memoria installata, ma deve consentire ad altri programmi installati nel sistema di poter essere allocati.

Ai programmi allocati e' consentito effettuare operazioni di lettura e scrittura solo dalla memoria centrale fisica.

Il problema che si vuole risolvere e' quello di realizzare la multiprogrammazione per supportare il multitasking.

#### Obiettivi
Gli obiettivi che si vogliono raggiungere sono:
* Ripartire la memoria centrale fisica tra i processi che sono stati caricati (incluso il sistema operativo) per realizzare sistemi multiprogrammati;
* Proteggere la memoria centrale da accessi non leciti da parte di un processo a zone occupate da altri processi (o dal sistema operativo).


#### Partizioni
E' possibile parizionare la memoria centrale in partizioni su cui verranno caricati i processi, lasciando sempre una porzione dedicata al sistema operativo.

Qualora il numero di partizioni disponibili fosse zero, e venisse lanciato un nuovo processo, questo dovra' attendere che una partizione venga liberata prima di poter essere allocato.

Se avessi un processo piu' grande della maggiore partizione presente, tale processo non potra' mai essere eseguito con quel partizionamento.

Nel partizionamento abbiamo dunque che:
> La somma dello spazio di indirizzamento dei processi e del sistema operativo e' uguale alla memoria centrale fisica.

#### Partizioni fisse (Statiche)
La tecnica delle partizioni fisse prevede che la memoria fisica venga suddivisa in un insieme di partizioni di grandezza predeterminata per i vari processi applicativi.

Queste partizioni vengono conservate in una **tabella delle partizioni** in cui vengono memorizzati gli indirizzi fisici di inzio delle varie partizioni.

Quando il sistema vorra' caricare un programma in una partizione andra' a scandire nella tabella delle partizioni quali tra queste e' libera e quale dispone di una dimensione sufficiente a contenere il programma desiderato.

Il partizionamento viene definito all'atto del **bootstrap**, e non potra' essere cambiato per tutta l'esecuzione dei programmi fin quando il sistema non viene spento.

#### Partizioni variabili (Dinamiche)
Un altro approccio e' quello delle partizini variabili per cui il numero delle partizioni e la dimensione di queste non viene definito in fase di bootstrap ma puo' essere cambiata **durante l'esecuzione**.

In questo caso abbiamo la **tabella delle partizioni** (che contiene le informazioni relative agli indirizzi fisici di inizio delle varie partizioni) che se durante l'esecuzione **ritiene opportuno modificare le partizioni**, ad esempio per caricare un programma che necessita di molto spazio che non riesce ad entrare in nessuna partizione, puo' **procedere con la ripartizione** (anche diminuendo il numero totale di partizioni disponibili) aggiornando conseguentemente la tabella.

#### Caratteristiche - Frammentazione
Il grosso problema che si ha con il partizionamento e' che rimangono spesso dei frammenti di memoria non utilizzati.

Ad esempio se un processo caricato in una partizione non la usa nella sua massima dimensione allocabile, ci sono delle zone di memoria completamente inutilizate ed inutilizzabili da altri processi.

Addirittura, la somma di tutta la memoria non utilizzata da vari processi, potrebbe consentire l'allocazione di un nuovo processo, ma poiche' questa e' bloccata dal partizionamento e non si trova in posizioni contigue cio' non e' possibile.

___

### Overlaying

#### Tecnica dell'overlaying della memoria centrale
Il problema fondamentale che si deve affrontare quando si vuole mandare in esecuzione un processo in un sistema di elaborazione e' che la memoria centrale fisica assegnata a tale processo puo' non bastare a contenere lo spazio di indirizzamento logico.

Il partizionamento consente di ripartire la memoria centrale fisica tra vari processi, consentendo quindi il multitasking, ma lo spazio di indirizzamento non cambia (puo' addirittura diminuire).

I problemi che possono generarsi sono:
* Un processo puo' eccedere la partizione di memoria a lui assegnata;
* Un processo, che trova uno spazio sufficiente per poter operare in un sistema, se _portato_ in un altro sistema, non ha uno spazio sufficiente (**portabilita'**).

#### Obiettivi
L'obiettivo dell'overlaying e' permettere ad un processo di avere unno spazio di indirizzamento logico piu' grande della memoria centrale fisica ad esso allocata, ma caricando in questa solo la **porzione** di spazio logico che serve per la computazione nell'immediato futuro.

Questa tecnica deve identificare le porzioni di codice e di dati che:
* Sono usate sempre;
* Sono usate in mutua esclusione per eseguire funzioni specifiche dell'applicazione (queste porzioni sono dette **overlay**).

Le porzioni di codice overlay devono essere caricate in memoria centrale solo quando deve essere eseguita la funzione richiesta.

Una volta individuate le porzioni di codice overlay bisogna creare in memoria centrale:
* Lo spazio per caricare le porzioni di codice che devono essere sempre residenti in memoria centrale;
* Lo spazio per caricare le porzioni di codice overlay.

Quando cambia la porzione di codice che serve nell'immediato futuro viene tolta la vecchia porzione di overlay e ne viene caricata un'altra.

E' necessario salvare dalla memoria centrale in uno spazio di memoria temporanea i dati modificati da un overlay caricato (consideriamo solo i dati globali e non quelli _generati_ dall'overlay - statefulness).
In questo modo e' possibile far spazio per i dati dei nuovi overlay da caricare.

![overlaying](md_resources/M1_overlaying.png)


#### Caratteristiche
> La tecnica dell'overlay _ripiega_ lo spazio logico occupato da un processo in uno spazio fisico piu' piccolo in memoria centrale.

La responsabilita' del programmatore e' identificare le porzioni di codice e di dati sovrapponibili tramite questa tecnica.
L'optimum e' che le porzioni di codice sovrapponibili abbiano delle **dimensioni omogenee** in modo da ridurre i delta di memoria centrale non utilizzata.

Il compilatore introdurra' le chiamate di caricamento e scaricamento in memoria centrale, collegandosi alla liberia di gestione dell'overlaying in modo tale da realizzare tale tecnica. Inserira' anche delle **operazioni di verifica** che accertino la presenza della funzionalita' richiesta dal processo all'interno dell'overlay caricato.

E' possibile realizzare degli **overlay multipli** che consentono di effettuare il caricamento e lo scaricamento massivo di porzioni di codice, ma anche di caricare la stessa porzione di codice piu' volte all'interno della memoria centrale.

Infine e' possibile realizzare degli **overlay gerarchici** per i quali una determinata porzione di memoria centrale in cui vi e' caricato un processo, viene _sottopartizionata_ che puo' essere utilizzata in overlay da uno degli overlay caricati globalmente di livello superiore.

___

### Swapping

#### Tecnica dello swapping della memoria centrale
Quando utilizziamo una tecnica di partizionamento in memoria centrale consentiamo la multiprogrammazione, e quindi aumenta il numero dei processi che sono caricati in memoria centrale, tutti nello stato di `Ready to Run` e che possono competere per l'uso del processore.
Se alcuni di questi programmi, entrano in uno stato di `Wait` (perche' devono attendere il completamento di un'operazione di I/O, o per altri motivi) questi continueranno comunque ad occupare spazio in memoria centrale, anche se non sono in grado di evolvere la propria computazione.
Questo spazio non e' utilizzabile, magari da altri processi, che non sono ancora stati caricati in memoria centrale: si ha dunque uno **spreco di memoria centrale**.

#### Obiettivi
L'obiettivo della tecnica di swapping e' quello di liberare lo spazio di memoria centrale fisica occupato da processi che sono in stato di `Wait` per far posto a processi che possono essere eseguiti: cio' aumenta il grado di multiprogrammazione tenendo in memoria centrale fisica solo processi nello stato di `Ready to Run` e `Running`.

#### Gestione
La tecnica dello swapping e' dunque composta da vari step:
* L'identificazione dei processi che non possono evolvere perche' sono in stato di `Wait`;
* Il salvataggio in una memoria temporanea (detta anche **swap**) di dati globali, heap e stack dei processi in stato di `Wait` (assumendo che il codice sia immutabile);
* La rimozione dalla memoria centrare fisica dei processi che si trovano nello stato di `Wait`;
* Il caricamento nello spazio che si e' liberato in memoria centrale di **un nuovo processo o un processo che era in memoria temponeanea** (quando ha ottenuto tutte le risorse informative o fisiche necessarie per riprendere la sua evoluzione).

![swapping](md_resources/M1_swapping.png)

La gestione e' completamente assegnata al sistema operativo, rendendola trasparente al programmatore.

#### Estensione dello swapping
La tecnica dello swapping ha anche altre applicazioni:
* Salvare lo stato di processi terminati per effettuare un'analisi a posteriori (addebito);
* Far ruotare dei processi in stato `Ready to Run` tra la memoria centrare fisica e l'area di swap aumentando la turnazione globale dei processi sul processore (_roll out/roll in_ per sistemi con schedulazione basata su priorita').

___

## Tecniche di base di secondo livello

### Paginazione
Le tecniche di base di primo livello soffrono di alcuni problemi di inefficienza:
* Caricare tutto il processo in memoria centrale e' inutile per l'evoluzione della computazione;
* Il caricamento e lo scaricamento di grosse porzioni di memoria richiedono tempi lunghi: lo swapping dell'intero processo e' lento;
* La gestione del caricamento e dello scaricamento effettuata dal programmatore nell'overlaying e' complessa e puo' portare a sprechi di memoria.

#### Obiettivi
Gli obiettivi della tecnica di paginazione sono:
* Mantenere in memoria centrale solo le porzioni del processo che servono nell'immediato futuro: **minima occupazione di memoria centrale**;
* Caricare e scaricare piccole porzioni di memoria: **velocita' di swapping e minimo sovraccarico**;
* Porzioni di memoria di dimensione identica: **efficienza di gestione**;
* Possibile non contiguita' delle porzioni di un processo nella memoria centrale fisica: **efficienza di gestione**;
* Gestione indipendente del programmatore: **correttezza, sicurezza ed equita'**.

#### Tecnica di paginazione
La tecnica di paginazione provvede a suddividere la memoria centrale fisica in **pagine fisiche (o frame)** tutte della stessa dimensione. Lo spazio di indirizzamento di ognuno dei processi viene analogamente diviso in porzioni dette **pagine logiche (o pagine)**. Le pagine fisiche e le pagine logiche hanno tutte la stessa dimensione.

La **tabella delle pagine** di un processo definisce la corrispondenza tra le pagine logiche e le pagine fisiche del processo considerato:
```
TabellaPagine[PaginaLogica] = PaginaFisica      (se caricata)
TabellaPagine[PaginaLogica] = ---               (se non caricata)
```

Implicitamente facciamo uso della tecnica di swapping per rimuovere dalla memoria centrale delle pagine fisiche che non servono piu' e caricare delle nuove pagine logiche per consentire l'evoluzione dei processi.

Gli indirizzi nelle pagine prendono la seguente forma:
* Indirizzo logico = numero di pagina logica *p*, spiazzamento nella pagina *d*;
* Indirizzo fisico = numero di pagina fisica *f*, spiazzamento nella pagina *d*.

![paginazione](md_resources/M1_paginazione.png)

* Il processore genera un indirizzo logico costituito da pagina logica *p* e spiazzamento *d*;
* Il numero di pagina logica viene usato come indice nella tabella delle pagine per trovare la pagina fisica *f* dove e' stata caricata;
* Il numero di pagina fisica *f* e lo spiazzamento *d* costituiranno l'indirizzo fisico della parola di memoria desiderata all'interno della memoria centrale.

#### Gestione
La gestione della paginazione prevede che:
* Solo le pagine logiche che servono all'immediato futuro per la computazione dei processi nello stato di pronto siano presenti in memoria centrale;
* Le pagine logiche di un processo possono essere caricate in pagine fisiche non contigue in memoria centrale fisica (perche' la tabella delle pagine permette di reperire la loro posizione fisica ovunque siano poste);
* Le pagine lofiche che non sono caricate in memoria centrale vengono conservate nell'area di swap;
* Le pagine fisiche modificate vengono salvate in area di swap prima di essere rimosse dalle pagine fisiche.

E' responsabilita' del sistema operativo gestire la paginazione in maniera trasparente per il programmatore. Si occupera' di:
* Selezionare le pagine da caricare in memoria centrale fisica;
* Caricare in memoria centrale le pagine necessarie ma non presenti (*page fault*);
* Selezionare le pagine da scaricare della memoria centrale fisica;
* Scaricare le pagine di memoria centrale non piu' necessarie.

#### Supporti
Questa tecnica prevedere il supporto del dispositivo hardware **Memory Management Unit** che contiene la tabella delle pagine o il suo indirizzo in memoria centrale, traducendo l'indirizzo logico in indirizzo fisico. Nel caso in cui una traduzione non puo' essere completata, la MMU segnalera' al processore la necessita' di gestire la situazione anomala tramite una `TRAP`.